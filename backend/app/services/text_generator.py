from typing import Sequence, TypedDict, Annotated, Optional
from langchain.chat_models import init_chat_model
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage
from langchain_core.tools import tool
from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode

# import os
import time
import json

from .platforms.blogger_service import postBlog
from .platforms.tweet_service import postTweet
from .platforms.reddit_service import postReddit
import asyncio
from scraper import search, search_and_scrape

# from dotenv import load_dotenv
# load_dotenv()

class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    tweet: Annotated[str, "â‰¤15 words, must include hashtags, mentions, emojis"]
    blog_post: Annotated[str, "â‰¥250 words, detailed and informative"]
    reddit_post: Annotated[str, "JSON string with 'title' and 'body'; 'body' supports Markdown"]
    input_index: int
       
def run_document_agent(inputs=None, auto=False, categories=None):  
    
    # time.sleep(2)   #   Pause at start 
    
    @tool 
    def update(tweet: str, blog_post: str, reddit_post: str) -> dict:
        """Updates the document with tweet, blog_post and reddit_post"""

        time.sleep(1)
        
        return {
            "tweet": tweet,
            "blog_post": blog_post,
            "reddit_post": reddit_post
        }

        
    @tool
    def save(filename: str, tweet: str, blog_post: str, reddit_post: str) -> str:
        """Save the Document in a text file and finish the process.

        Args:
            filename (str): Name of the text file.
            tweet (str): A short message (max 15 words) including hashtags (#), mentions (@), w
                        and emojis ðŸ˜ƒðŸ”¥âœ¨ when relevant.
                        
            blog_post (str): A detailed and informative blog_post of atleast 250 words, expanding on the tweets theme.
            reddit_post (str): JSON string with 'title' and 'body'; 'body' supports Markdown
        """
        
        time.sleep(1)
        
        if not filename.endswith(".txt"):
            filename = f"{filename}.txt"
            
        #post code --> write later (check the returns)
        try:
            postTweet(tweet)
            print("Tweet posted successfully!")
        except Exception as e:
            print(f"Tweet posting failed: {str(e)}")
            
        try:
            postBlog(blog_post)
            print("Blog posted successfully!")
        except Exception as e:
            print(f"Blog posting failed: {str(e)}")
            
        try:
            reddit_post_dict = json.loads(reddit_post)
            title = reddit_post_dict['title']
            body = reddit_post_dict['body']
  
            postReddit(title=title, text=body)
            print("Reddit posted Successfully!")
                
        except Exception as e:
            print(f"Reddit posting failed: {str(e)}")
            
        return f"Document has been saved successfully to {filename}"
    
        #   Save file code
        # DATA_PATH = r"C:\Users\rvisw\OneDrive\Desktop\shashank_Project\content_creation_agent\data\final_ouput"
        # FILE_PATH = os.path.join(DATA_PATH, filename)
        
        
        # try: 
        #     document_content = f"Tweet: \n{tweet}\nBlog Post: \n{blog_post}\n Reddit Post: \n{reddit_post}\n"
        #     with open(FILE_PATH, "w", encoding="utf-8") as file:
        #         file.write(document_content)
        #     print(f"\n Document saved to {filename}")
        #     return f"Document has been saved successfully to {filename}"
        # except Exception as e:
        #     return f"File not saved due to error: {str(e)}"
        
              
    @tool    
    def search_tool(query: str) -> str:
        """
        This tool searches and returns information from the web using search api and scraping.
        """
        
        time.sleep(1)
        
        data = asyncio.run(search(query, categories=categories))
        scraped_data = asyncio.run(search_and_scrape(query, categories=categories, data=data, maxURL=5))
        
        # data["results"] -> format
        # "url": "https://finance.yahoo.com/news/buy-cryptocurrency-xrp-while-under-091500196.html?fr=sycsrp_catchall",
        # "title": "Should You Buy Cryptocurrency XRP While It's Under $3?",
        # "content": "This has always been the core of XRP's investing thesis: As more banks adopt the technology, it will..."
        
        #   If you want to mention source: output = f"Sources: {},\ncontent:{content}" return this.
        
        results = []
        results.append("Search Results: \n")
        for result in data["results"]:
            if result.get("title") and result.get("content"):
                results.append(f'title: {result["title"]}\ncontent: {result["content"]}\n')
        
        results.append("Scraped Results: \n")
        for result in scraped_data:         #   If you want first search to be latest (last in content) use "reversed(scraped_data)"
            if result.get('text'):
                results.append(f'title: {result["title"]}\ncontent: {result["text"]}\n')
        
        content = "\n\n".join(results)
        
        if not content:
            return f"Information not found in the retrieved content."
        
        return content
        
        
    tools = [search_tool, update, save]
    llm = init_chat_model('gemini-2.5-flash', model_provider="google_genai").bind_tools(tools)

    #   Function to automate user input
    def get_User_or_Auto_input(state: AgentState, config=None) -> HumanMessage :
        
        cfg = config.get("configurable", {}) if config else {}
        
        if cfg.get("auto_mode"):
            inputs = cfg.get("inputs")
            current_idx = state.get("input_index", 0)
            user_message = HumanMessage(content="Save it")  # Default (if not saved by previous command)
            
            if current_idx < len(inputs):
                user_input = inputs[current_idx]
                user_message = HumanMessage(content=user_input)
        
        # if AUTO_MODE:
        #     user_input = f"Save it."
        #     user_message = HumanMessage(content=user_input)
        
        elif not state["messages"]:
            user_input = "I'm ready to help you update the document. What would you like to create?"
            user_message = HumanMessage(content=user_input)
            
        else:
            user_input = input("\n How would you like to update the document?")
            print(f"\n USER: {user_input}")
            user_message = HumanMessage(content=user_input)
            
        return user_message


    def our_agent(state: AgentState, config=None) -> AgentState:
        
        system_prompt = SystemMessage(content=f"""
        You are an Intelligent AI Assistant. Your role is to retrieve information using `search_tool`, 
        then generate, update, or save content in strict compliance with the JSON schema below:

        {{
            "tweet": [
                "Must include hashtags (#), mentions (@), and emojis ðŸ˜ƒðŸ”¥âœ¨ when relevant",
                "Maximum of 15 words",
                "Concise, catchy, and engaging"
            ],
            "blog_post": [
                "At most 1000 words",
                "Detailed, informative, and engaging", 
                "Should expand on the Tweet's theme with depth and clarity",
                "Blog post must be written in valid HTML format with proper tags (<h1>, <p>, <ul>, etc.)"
            ],
            "reddit_post": [
                "JSON string with keys 'title' and 'body'",
                "'title': short and catchy (â‰¤300 characters)",
                "'body': detailed text with Reddit Markdown (not HTML)",
                "This should only contain 'title' and 'body'.Do not include extra fields "
                "Body may include bullet points, lists, and formatting relevant to discussion",
                "can be empty if not applicable"
                "Add relevant subreddit flair if appropriate(optional)",
                "End with a question or call to discussion to encourage comments(optional)"
            ]
        }}

        ### Rules:
        - Always call `search_tool` first to gather context, unless the answer is already fully in the current document.
        - The query sent to search_tool must be a concise, summarized version of the user input, formatted as a search-friendly string or set of keywords, suitable for search engines like Google, Bing, or Qwant.
        - **After retrieving documents**, always generate both `tweet`, `blog_post` and `reddit_post` and call the `update` tool with the full updated content
        - Always use **tools** (`update`, `save`) for any JSON output. Do NOT directly print JSON in your final reply.
        - `update` must include both the full updated "tweet", "blog_post" and "reddit_post".
        - `save` saves both tweet and blog post content together.
        - **IMPORTANT**: After calling `save`, wait for the tool result. If you see "Document has been saved successfully", the process is complete.
        - Always generate a descriptive filename ending with `.txt` (e.g., `"tweet_marketing.txt"`, `"blog_ai_trends.txt"`).
        - Filenames must reflect the document content meaningfully.
        - Cite specific parts of retrieved information in the blog post.
        - After `update` or `save`, always pass the **entire current state** of the document as tool arguments.
        - If API posting fails but file is saved, consider it a success.

        ### Current Document:
        {{
            "tweet": "{state.get('tweet', '')}",
            "blog_post": "{state.get('blog_post', '')}",
            "reddit_post": "{state.get('reddit_post', '')}"
        }}
    """)
        
        
        user_message = get_User_or_Auto_input(state, config)

        prompt = [system_prompt] + list(state["messages"]) + [user_message]
        
        response = llm.invoke(prompt)
        
        print("\nCurrent tweet:", state['tweet'])
        print("\nCurrent blog_post:", state['blog_post'])
        print("\nCurrent reddit_post:", state['reddit_post'])
        
        print(f"\n AI: {response.content}")
        if hasattr(response, "tool_calls") and response.tool_calls:
            print(f"\n Using TOOLS: {[tc for tc in response.tool_calls]}")

        return {
            "messages": list(state['messages']) + [user_message, response], 
            "input_index": state["input_index"]+1, 
        }

    def should_continue(state: AgentState):
        """Decides whether we continue or end the conversation"""
        messages = state['messages']
        
        if not messages: 
            return "continue"
        #   End if Saved
        for message in reversed(messages):
            if(isinstance(message, ToolMessage)):
            
                if ("saved successfully" in message.content.lower() or ("document has been saved" in message.content.lower())):
                    print("Found save confirmation - Ending...")  
                    return "end"
            
                if ("File not saved" in message.content.lower()):
                    print("Save Error - Ending....")
                    return "end"
                #   End if no retrieved data
            
                if ("information not found" in message.content.lower() and "retrieved content" in message.content.lower()):
                    print("No data found - Ending.....")  
                    return "end"
        
        return "continue"

    def print_messages(messages):
        """Funtion I made to print messages in a readable format"""
        if not messages: 
            return
        
        for message in messages[-3:]:
            if isinstance(message, ToolMessage):
                print(f"\n TOOL RESULT: {message.content}")

    def create_stateful_tool_node(tools):
        
        def stateful_tool_node(state: AgentState):
            tool_node = ToolNode(tools)
            result = tool_node.invoke(state)
            
            updated_tweet = ""
            updated_blog_post = ""
            updated_reddit_post = ""
        
            if state["messages"]:
                last_ai_message = state["messages"][-1]
                if hasattr(last_ai_message, "tool_calls"):
                    for tool_call in last_ai_message.tool_calls:
                        if tool_call["name"] in ["update", "save"]:
                            tool_args = tool_call["args"]
                            updated_tweet = tool_args["tweet"]
                            updated_blog_post = tool_args["blog_post"]
                            updated_reddit_post = tool_args["reddit_post"]
                            
            return {
                "messages": result["messages"],
                "tweet": updated_tweet,
                "blog_post": updated_blog_post,
                "reddit_post": updated_reddit_post
            }            
            
        return stateful_tool_node
                
    graph = StateGraph(AgentState)

    graph.add_node("tools", create_stateful_tool_node(tools))
    graph.add_node("agent", our_agent)

    graph.add_edge(START, "agent")
    graph.add_edge("agent", "tools")

    graph.add_conditional_edges(
        "tools",
        should_continue,
        {
            "continue": "agent",
            "end": END
        }
    )

    app = graph.compile()


    print("\n *****DRAFTER*****")
    
    # Toggle Automatic
    
    config = None
    
    if inputs and auto:
        config = {
            "configurable": {
                "inputs": inputs,
                "auto_mode": auto
            }
        }
        
    state = {"messages": [], 'tweet':"", 'blog_post':"", 'reddit_post':"", "input_index":0}

    final_state = None
    
    for step in app.stream(state, config=config, stream_mode="values"):
        if "messages" in step:
            print_messages(step["messages"])
        final_state = step
        
    print("\n***FINISHED DRAFTER***")
    return final_state

if __name__ == "__main__":
    run_document_agent()
            


